{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File for Prompt Evaluation. \n",
    "#### Using Levenshtein Distance, BLEU/ROUGE and XML Validation for Scoring Prompt Candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install levenshtein\n",
    "#!pip3 install nltk\n",
    "#!pip3 install rouge-score\n",
    "#!pip3 install langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    print(\"OPEN AI API KEY IS MISSING\")\n",
    "\n",
    "# Initializing ChatGPT/Openai\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2\n",
    "    # base_url=\"...\",\n",
    "    # organization=\"...\",\n",
    "    # other params...\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading Process Descs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_process_desc_nebentaetigkeiten = open(\"assets/process_desc_nebentaetigkeiten.txt\", encoding=\"utf8\")\n",
    "file_process_desc_debriefing = open(\"assets/process_desc_debriefing.txt\", encoding=\"utf8\")\n",
    "file_process_desc_bedarfsermittlung = open(\"assets/process_desc_bedarfsermittlung.txt\", encoding=\"utf8\")\n",
    "file_input_example_1 = open(\"assets/example_for_ai_1.drawio\", encoding=\"utf8\")\n",
    "file_input_example_2 = open(\"assets/example_for_ai_2.drawio\", encoding=\"utf8\")\n",
    "\n",
    "# save process descriptions\n",
    "process_desc_nebentaetigkeiten = file_process_desc_nebentaetigkeiten.read()\n",
    "process_desc_debriefing = file_process_desc_debriefing.read()\n",
    "process_desc_bedarfsermittlung = file_process_desc_bedarfsermittlung.read()\n",
    "example_input_file_1 = file_input_example_1\n",
    "example_input_file_2 = file_input_example_2\n",
    "\n",
    "# close files\n",
    "file_process_desc_nebentaetigkeiten.close()\n",
    "file_process_desc_debriefing.close()\n",
    "file_process_desc_bedarfsermittlung.close()\n",
    "file_input_example_1.close()\n",
    "file_input_example_2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompts for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Prompt Extracting Roles\n",
    "prompt_extracting_roles = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            'system',\n",
    "            'Du bist ein Prozessmanager extrahierst Rollen aus einer Prozessbeschreibung heraus.',\n",
    "        ),\n",
    "        (\"human\", 'Extrahiere aus folgender Prozessbeschreibung alle Beteiligten Rollen und gib diese als nicht Liste in folgendem Format zurück. Format: [\"Eintrag1\", \"Eintrag2\", \"Eintrage3\"],  Prozessbeschreibung: \"{prozessbeschreibung}\"'),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Prompt Extracting Activities\n",
    "prompt_extracting_activities = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            'system',\n",
    "            'Du bist ein Prozessmanager extrahierst Rollen aus einer Prozessbeschreibung heraus.',\n",
    "        ),\n",
    "        (\"human\", 'Extrahiere aus folgender Prozessbeschreibung alle Beteiligten Rollen und gib diese als Liste zurück. Prozessbeschreibung: \"{prozessbeschreibung}\"'),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Prompt creating Model\n",
    "prompt_creating_model = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            'system',\n",
    "            'Du bist ein Prozessmanager extrahierst Rollen aus einer Prozessbeschreibung heraus.',\n",
    "        ),\n",
    "        (\"human\", 'Extrahiere aus folgender Prozessbeschreibung alle Beteiligten Rollen und gib diese als Liste zurück. Prozessbeschreibung: \"{prozessbeschreibung}\"'),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialising Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestCaseExtraction:\n",
    "    def __init__(self, case_name, process_desc, ground_truth, prompt):\n",
    "        self.case_name = case_name\n",
    "        self.process_desc = process_desc\n",
    "        self.ground_truth = ground_truth\n",
    "        self.prompt = prompt\n",
    "\n",
    "class TestCaseModelCreation:\n",
    "    def __init__(self, case_name, process_desc, input_1, input_2, ground_truth, prompt):\n",
    "        self.case_name = case_name\n",
    "        self.input_1 = input_1\n",
    "        self.input_2 = input_2\n",
    "        self.process_desc = process_desc\n",
    "        self.ground_truth = ground_truth\n",
    "        self.prompt = prompt\n",
    "\n",
    "# Test Cases for Role Extraction\n",
    "test_case_roles_bedarfsermittlung = [\"\"]\n",
    "test_case_roles_debriefing = TestCaseExtraction(\"Debriefing_role_extraction\", process_desc_debriefing, [\"HR\"], prompt_extracting_roles)\n",
    "test_case_roles_nebentaetigkeiten = TestCaseExtraction(\"Nebentaetigkeiten_role_extraction\", process_desc_nebentaetigkeiten, [\"Mitarbeiter\", \"HR\"], prompt_extracting_roles)\n",
    "\n",
    "list_test_cases_role_extraction = [test_case_roles_debriefing, test_case_roles_nebentaetigkeiten]\n",
    "\n",
    "# Test Cases for Activity Extraction\n",
    "activities_bedarfsermittlung = [\"\"]\n",
    "\n",
    "activity_list_debriefing = [\"\"]\n",
    "test_case_activities_debriefing = TestCaseExtraction(\"Debriefing_activity_extraction\", process_desc_debriefing, activity_list_debriefing, prompt_extracting_roles)\n",
    "\n",
    "activity_list_nebentaetigkeiten = [\"\"]\n",
    "test_case_activities_nebentaetigkeiten = TestCaseExtraction(\"Nebentaetigkeiten_activity_extraction\", process_desc_nebentaetigkeiten, activity_list_nebentaetigkeiten, prompt_extracting_roles)\n",
    "\n",
    "list_test_cases_activity_extraction = [test_case_activities_debriefing, test_case_activities_nebentaetigkeiten]\n",
    "\n",
    "# Test Cases for Model Creation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bleu, Rouge and Meteor Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'roles_nebentaetigkeiten' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrouge_score\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rouge_scorer\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Bleu Score for 2-grams\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m nltk\u001b[38;5;241m.\u001b[39mtranslate\u001b[38;5;241m.\u001b[39mbleu_score\u001b[38;5;241m.\u001b[39msentence_bleu([\u001b[43mroles_nebentaetigkeiten\u001b[49m], roles_extracted,(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Rouge Score\u001b[39;00m\n\u001b[0;32m     11\u001b[0m scorer \u001b[38;5;241m=\u001b[39m rouge_scorer\u001b[38;5;241m.\u001b[39mRougeScorer([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrouge1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrouge2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrougeL\u001b[39m\u001b[38;5;124m'\u001b[39m], use_stemmer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'roles_nebentaetigkeiten' is not defined"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import nltk.translate.bleu_score\n",
    "\n",
    "import nltk.translate.meteor_score\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Bleu Score for 2-grams\n",
    "nltk.translate.bleu_score.sentence_bleu([roles_nebentaetigkeiten], roles_extracted,(1,0,0,0))\n",
    "\n",
    "# Rouge Score\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "# Converting List to String for Rouge\n",
    "string1 = ' '.join(roles_nebentaetigkeiten)\n",
    "string2 = ' '.join(roles_extracted)\n",
    "\n",
    "# Rouge Scoring\n",
    "scores = scorer.score(string1, string2)\n",
    "for key in scores:\n",
    "    print(f'{key}: {scores[key]}')\n",
    "    \n",
    "print(string1)\n",
    "print(string2)\n",
    "\n",
    "# Meteor Score\n",
    "nltk.translate.meteor_score.meteor_score([roles_nebentaetigkeiten], roles_extracted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Role Extraction Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Scheske\\Documents\\Private\\Studium Ohm\\Bachelorarbeit\\Repo\\Bachelorarbeit\\.venv\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\Users\\Scheske\\Documents\\Private\\Studium Ohm\\Bachelorarbeit\\Repo\\Bachelorarbeit\\.venv\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\Users\\Scheske\\Documents\\Private\\Studium Ohm\\Bachelorarbeit\\Repo\\Bachelorarbeit\\.venv\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\Users\\Scheske\\Documents\\Private\\Studium Ohm\\Bachelorarbeit\\Repo\\Bachelorarbeit\\.venv\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\Users\\Scheske\\Documents\\Private\\Studium Ohm\\Bachelorarbeit\\Repo\\Bachelorarbeit\\.venv\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case Name</th>\n",
       "      <th>Bleu Score</th>\n",
       "      <th>Rouge 1 Score</th>\n",
       "      <th>Rouge L Score</th>\n",
       "      <th>Meteor Score</th>\n",
       "      <th>Prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Debriefing_role_extraction</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>3</td>\n",
       "      <td>((name, None), (input_variables, [prozessbesch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nebentaetigkeiten_role_extraction</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>3</td>\n",
       "      <td>((name, None), (input_variables, [prozessbesch...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Case Name  Bleu Score  Rouge 1 Score  Rouge L Score  Meteor Score                                             Prompt\n",
       "0         Debriefing_role_extraction    0.250000       0.333333       0.333333             3  ((name, None), (input_variables, [prozessbesch...\n",
       "1  Nebentaetigkeiten_role_extraction    0.666667       0.571429       0.571429             3  ((name, None), (input_variables, [prozessbesch..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import Levenshtein as lev\n",
    "import ast\n",
    "import pandas as pd\n",
    "\n",
    "#roles_nebentaetigkeiten = []\n",
    "#roles_nebentaetigkeiten.sort()\n",
    "#roles_extracted.sort()\n",
    "\n",
    "#lev.distance(roles_nebentaetigkeiten, roles_extracted)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "df = pd.DataFrame(columns=[\"Case Name\", \"Bleu Score\", \"Rouge 1 Score\", \"Rouge L Score\", \"Meteor Score\", \"Prompt\"])\n",
    "\n",
    "# init rouge scorer with only 1 grams and RougeL\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "for case in list_test_cases_role_extraction:\n",
    "\n",
    "    chain = case.prompt | llm\n",
    "    response_role_extraction = chain.invoke({\n",
    "        \"prozessbeschreibung\" : case.process_desc\n",
    "    })\n",
    "    \n",
    "    # Convertion to string list\n",
    "    extracted_roles = ast.literal_eval(response_role_extraction.content)\n",
    "    #print(extracted_roles)\n",
    "    \n",
    "    extracted_roles.sort()\n",
    "    case.ground_truth.sort()\n",
    "    \n",
    "    # Calc Bleu Score with only 1 grams for roles\n",
    "    bleu_score = nltk.translate.bleu_score.sentence_bleu([case.ground_truth], extracted_roles, (1, 0, 0, 0))\n",
    "    \n",
    "    # Calc Rouge Score\n",
    "    scores = scorer.score(str(case.ground_truth), str(extracted_roles))\n",
    "    rouge1_score = scores[\"rouge1\"].fmeasure\n",
    "    rougeL_score = scores[\"rougeL\"].fmeasure\n",
    "    \n",
    "    \n",
    "\n",
    "    df.loc[len(df)] =(case.case_name, bleu_score, rouge1_score, rougeL_score, 3, case.prompt)\n",
    "    \n",
    "    \n",
    "    \n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Case Name  Bleu Score\n",
      "0         Debriefing_role_extraction    0.250000\n",
      "1  Nebentaetigkeiten_role_extraction    0.666667\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Scheske\\Documents\\Private\\Studium Ohm\\Bachelorarbeit\\Repo\\Bachelorarbeit\\.venv\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\Users\\Scheske\\Documents\\Private\\Studium Ohm\\Bachelorarbeit\\Repo\\Bachelorarbeit\\.venv\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\Users\\Scheske\\Documents\\Private\\Studium Ohm\\Bachelorarbeit\\Repo\\Bachelorarbeit\\.venv\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "print(nltk.translate.bleu_score.sentence_bleu([['HR']], [\"HR\", \"Mitarbeiter/in\", \"Abteilungen\", \"GF\"], (1, 0, 0, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
