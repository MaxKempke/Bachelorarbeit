{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File for Prompt Evaluation. \n",
    "#### Using Levenshtein Distance, BLEU/ROUGE and XML Validation for Scoring Prompt Candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge-score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting absl-py (from rouge-score)\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: nltk in c:\\users\\maxim\\documents\\studium\\bachelorarbeit\\python repo\\.venv\\lib\\site-packages (from rouge-score) (3.9.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\maxim\\documents\\studium\\bachelorarbeit\\python repo\\.venv\\lib\\site-packages (from rouge-score) (2.1.3)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\maxim\\documents\\studium\\bachelorarbeit\\python repo\\.venv\\lib\\site-packages (from rouge-score) (1.16.0)\n",
      "Requirement already satisfied: click in c:\\users\\maxim\\documents\\studium\\bachelorarbeit\\python repo\\.venv\\lib\\site-packages (from nltk->rouge-score) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\maxim\\documents\\studium\\bachelorarbeit\\python repo\\.venv\\lib\\site-packages (from nltk->rouge-score) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\maxim\\documents\\studium\\bachelorarbeit\\python repo\\.venv\\lib\\site-packages (from nltk->rouge-score) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\maxim\\documents\\studium\\bachelorarbeit\\python repo\\.venv\\lib\\site-packages (from nltk->rouge-score) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\maxim\\documents\\studium\\bachelorarbeit\\python repo\\.venv\\lib\\site-packages (from click->nltk->rouge-score) (0.4.6)\n",
      "Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Building wheels for collected packages: rouge-score\n",
      "  Building wheel for rouge-score (pyproject.toml): started\n",
      "  Building wheel for rouge-score (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=25025 sha256=af642edaa5f42fd45cfca649627be1ec5fb956d8f524ac7f2f39948474331789\n",
      "  Stored in directory: c:\\users\\maxim\\appdata\\local\\pip\\cache\\wheels\\85\\9d\\af\\01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n",
      "Successfully built rouge-score\n",
      "Installing collected packages: absl-py, rouge-score\n",
      "Successfully installed absl-py-2.1.0 rouge-score-0.1.2\n"
     ]
    }
   ],
   "source": [
    "#!pip3 install levenshtein\n",
    "#!pip3 install nltk\n",
    "#!pip3 install rouge-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    print(\"OPEN AI API KEY IS MISSING\")\n",
    "\n",
    "# Initializing ChatGPT/Openai\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2\n",
    "    # base_url=\"...\",\n",
    "    # organization=\"...\",\n",
    "    # other params...\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading Process Descs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_process_desc_nebentaetigkeiten = open(\"assets/process_desc_nebentaetigkeiten.txt\", encoding=\"utf8\")\n",
    "file_process_desc_debriefing = open(\"assets/process_desc_debriefing.txt\", encoding=\"utf8\")\n",
    "file_process_desc_bedarfsermittlung = open(\"assets/process_desc_bedarfsermittlung.txt\", encoding=\"utf8\")\n",
    "file_input_example_1 = open(\"assets/example_for_ai_1.drawio\", encoding=\"utf8\")\n",
    "file_input_example_2 = open(\"assets/example_for_ai_2.drawio\", encoding=\"utf8\")\n",
    "\n",
    "# save process descriptions\n",
    "process_desc_nebentaetigkeiten = file_process_desc_nebentaetigkeiten.read()\n",
    "process_desc_debriefing = file_process_desc_debriefing.read()\n",
    "process_desc_bedarfsermittlung = file_process_desc_bedarfsermittlung.read()\n",
    "example_input_file_1 = file_input_example_1\n",
    "example_input_file_2 = file_input_example_2\n",
    "\n",
    "# close files\n",
    "file_process_desc_nebentaetigkeiten.close()\n",
    "file_process_desc_debriefing.close()\n",
    "file_process_desc_bedarfsermittlung.close()\n",
    "file_input_example_1.close()\n",
    "file_input_example_2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompts for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Prompt Extracting Roles\n",
    "prompt_extracting_roles = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            'system',\n",
    "            'Du bist ein Prozessmanager extrahierst Rollen aus einer Prozessbeschreibung heraus.',\n",
    "        ),\n",
    "        (\"human\", 'Extrahiere aus folgender Prozessbeschreibung alle Beteiligten Rollen und gib diese als Liste zurück. Prozessbeschreibung: \"{prozessbeschreibung}\"'),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Prompt Extracting Activities\n",
    "prompt_extracting_activities = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            'system',\n",
    "            'Du bist ein Prozessmanager extrahierst Rollen aus einer Prozessbeschreibung heraus.',\n",
    "        ),\n",
    "        (\"human\", 'Extrahiere aus folgender Prozessbeschreibung alle Beteiligten Rollen und gib diese als Liste zurück. Prozessbeschreibung: \"{prozessbeschreibung}\"'),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Prompt creating Model\n",
    "prompt_creating_model = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            'system',\n",
    "            'Du bist ein Prozessmanager extrahierst Rollen aus einer Prozessbeschreibung heraus.',\n",
    "        ),\n",
    "        (\"human\", 'Extrahiere aus folgender Prozessbeschreibung alle Beteiligten Rollen und gib diese als Liste zurück. Prozessbeschreibung: \"{prozessbeschreibung}\"'),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialising Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestCaseExtraction:\n",
    "    def __init__(self, case_name, process_desc, ground_truth, prompt):\n",
    "        self.case_name = case_name\n",
    "        self.process_desc = process_desc\n",
    "        self.ground_truth = ground_truth\n",
    "        self.prompt = prompt\n",
    "\n",
    "class TestCaseModelCreation:\n",
    "    def __init__(self, case_name, process_desc, input_1, input_2, ground_truth, prompt):\n",
    "        self.case_name = case_name\n",
    "        self.input_1 = input_1\n",
    "        self.input_2 = input_2\n",
    "        self.process_desc = process_desc\n",
    "        self.ground_truth = ground_truth\n",
    "        self.prompt = prompt\n",
    "\n",
    "# Test Cases for Role Extraction\n",
    "test_case_roles_bedarfsermittlung = [\"\"]\n",
    "test_case_roles_debriefing = TestCaseExtraction(\"Debriefing_role_extraction\", process_desc_debriefing, [\"HR\"])\n",
    "test_case_roles_nebentaetigkeiten = TestCaseExtraction(\"Nebentaetigkeiten_role_extraction\", process_desc_nebentaetigkeiten, [\"Mitarbeiter\", \"HR\"])\n",
    "\n",
    "list_test_cases_role_extraction = [test_case_roles_debriefing, test_case_roles_nebentaetigkeiten]\n",
    "\n",
    "# Test Cases for Activity Extraction\n",
    "activities_bedarfsermittlung = [\"\"]\n",
    "\n",
    "activity_list_debriefing = [\"\"]\n",
    "test_case_activities_debriefing = TestCaseExtraction(\"Debriefing_activity_extraction\", process_desc_debriefing, activity_list_debriefing)\n",
    "\n",
    "activity_list_nebentaetigkeiten = [\"\"]\n",
    "test_case_activities_nebentaetigkeiten = TestCaseExtraction(\"Nebentaetigkeiten_activity_extraction\", process_desc_nebentaetigkeiten, activity_list_nebentaetigkeiten)\n",
    "\n",
    "list_test_cases_activity_extraction = [test_case_activities_debriefing, test_case_activities_nebentaetigkeiten]\n",
    "\n",
    "# Test Cases for Model Creation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bleu, Rouge and Meteor Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1: Score(precision=1.0, recall=1.0, fmeasure=1.0)\n",
      "rouge2: Score(precision=1.0, recall=1.0, fmeasure=1.0)\n",
      "rougeL: Score(precision=1.0, recall=1.0, fmeasure=1.0)\n",
      "HR Mitarbeiter\n",
      "HR Mitarbeiter\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9375"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import nltk.translate.bleu_score\n",
    "\n",
    "import nltk.translate.meteor_score\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Bleu Score for 2-grams\n",
    "nltk.translate.bleu_score.sentence_bleu([roles_nebentaetigkeiten], roles_extracted,(0.5,0.5))\n",
    "\n",
    "# Rouge Score\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "# Converting List to String for Rouge\n",
    "string1 = ' '.join(roles_nebentaetigkeiten)\n",
    "string2 = ' '.join(roles_extracted)\n",
    "\n",
    "# Rouge Scoring\n",
    "scores = scorer.score(string1, string2)\n",
    "for key in scores:\n",
    "    print(f'{key}: {scores[key]}')\n",
    "    \n",
    "print(string1)\n",
    "print(string2)\n",
    "\n",
    "# Meteor Score\n",
    "nltk.translate.meteor_score.meteor_score([roles_nebentaetigkeiten], roles_extracted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Role Extraction Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein as lev\n",
    "\n",
    "roles_nebentaetigkeiten.sort()\n",
    "roles_extracted.sort()\n",
    "\n",
    "lev.distance(roles_nebentaetigkeiten, roles_extracted)\n",
    "\n",
    "chain = prompt_extracting_roles | llm\n",
    "response_role_extraction = chain.invoke({\n",
    "    \"prozessbeschreibung\" :\n",
    "})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
